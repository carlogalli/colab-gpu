{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Julia 1.7",
      "language": "julia",
      "name": "julia-1.7"
    },
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia",
      "version": "1.7.2"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carlogalli/colab-gpu/blob/main/cuda_structures_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installation"
      ],
      "metadata": {
        "id": "tPIu4SvlIuWW"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMGwZ7aFJL8Y"
      },
      "source": [
        "# Installation cell\n",
        "%%capture\n",
        "%%shell\n",
        "if ! command -v julia 3>&1 > /dev/null\n",
        "then\n",
        "    wget -q 'https://julialang-s3.julialang.org/bin/linux/x64/1.7/julia-1.7.2-linux-x86_64.tar.gz' \\\n",
        "        -O /tmp/julia.tar.gz\n",
        "    tar -x -f /tmp/julia.tar.gz -C /usr/local --strip-components 1\n",
        "    rm /tmp/julia.tar.gz\n",
        "fi\n",
        "julia -e 'using Pkg; pkg\"add IJulia; precompile;\"'\n",
        "echo 'Done'"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdMpcQduyaQc"
      },
      "source": [
        "After you run the first cell (the the cell directly above this text), go to Colab's menu bar and select **Edit** and select **Notebook settings** from the drop down. Select *Julia 1.7* in Runtime type. You can also select your prefered harwdware acceleration (defaults to GPU)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIxu4TjlJnBG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e37f77a6-c94f-4468-dd91-fa71120a6a3e"
      },
      "source": [
        "# print Julia version\n",
        "VERSION"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "v\"1.7.2\""
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "using Pkg\n",
        "Pkg.add([\"CUDA\", \"Random\", \"Printf\", \"BenchmarkTools\", \"Adapt\"]);\n",
        "ENV[\"JULIA_CUDA_USE_BINARYBUILDER\"] = false\n",
        "using Random, CUDA, Printf, BenchmarkTools, Adapt;\n",
        "\n",
        "function print_gpu_properties()\n",
        "\n",
        "    for (i,device) in enumerate(CUDA.devices())\n",
        "        println(\"*** General properties for device $i ***\")\n",
        "        name = CUDA.name(device)\n",
        "        println(\"Device name: $name\")\n",
        "        major = CUDA.attribute(device, CUDA.CU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MAJOR)\n",
        "        minor = CUDA.attribute(device, CUDA.CU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MINOR)\n",
        "        println(\"Compute capabilities: $major.$minor\")\n",
        "        clock_rate = CUDA.attribute(device, CUDA.CU_DEVICE_ATTRIBUTE_CLOCK_RATE)\n",
        "        println(\"Clock rate: $clock_rate\")\n",
        "        device_overlap = CUDA.attribute(device, CUDA.CU_DEVICE_ATTRIBUTE_GPU_OVERLAP)\n",
        "        print(\"Device copy overlap: \")\n",
        "        println(device_overlap > 0 ? \"enabled\" : \"disabled\")\n",
        "        kernel_exec_timeout = CUDA.attribute(device, CUDA.CU_DEVICE_ATTRIBUTE_KERNEL_EXEC_TIMEOUT)\n",
        "        print(\"Kernel execution timeout: \")\n",
        "        println(kernel_exec_timeout > 0 ? \"enabled\" : \"disabled\")\n",
        "        # a = CUDA.attribute(device, CUDA.CU_DEVICE_ATTRIBUTE_MAX_BLOCK_DIM_X)\n",
        "        # d = CUDA.attribute(device, CUDA.CU_DEVICE_ATTRIBUTE_MAX_GRID_DIM_X)       \n",
        "        a = CUDA.attribute(device, CUDA.CU_DEVICE_ATTRIBUTE_MULTIPROCESSOR_COUNT)\n",
        "        println(\"Number of multiprocessors: $a\")\n",
        "        b = CUDA.attribute(device, CUDA.CU_DEVICE_ATTRIBUTE_MAX_BLOCKS_PER_MULTIPROCESSOR)\n",
        "        println(\"Max blocks per MP: $b\")\n",
        "        c = CUDA.attribute(device, CUDA.CU_DEVICE_ATTRIBUTE_MAX_THREADS_PER_BLOCK)        \n",
        "        println(\"Max threads per block: $c\")\n",
        "        \n",
        "        println([a b c a*b*c])\n",
        "    end\n",
        "end\n",
        "print_gpu_properties()\n",
        "# with the falseENV option it takes 117.357304 seconds (35.94 M allocations: 2.301 GiB, 1.29% gc time, 11.71% compilation time)\n",
        "# without the falseENV option it takes  124.465413 seconds (39.74 M allocations: 2.537 GiB, 1.27% gc time, 12.65% compilation time)"
      ],
      "metadata": {
        "id": "4NaxNh3oPVUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "function test1(vin, vout)\n",
        "    i = threadIdx().x\n",
        "    vout[i] = vin[i]*10\n",
        "    return nothing\n",
        "end"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNaJAd5SPfj-",
        "outputId": "a417c669-2b3d-4cee-d30d-a6dfb97f5379"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "test1 (generic function with 2 methods)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n = 10\n",
        "xin = CuArray(rand(n))\n",
        "xout = CuArray(zeros(n))\n",
        "@cuda threads=n test1(xin,xout)\n",
        "[xin xout]"
      ],
      "metadata": {
        "id": "aP7TGLzbPyfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "function test2(s)\n",
        "    i = threadIdx().x\n",
        "    s.vout[i] = s.vin[i]*10\n",
        "    return nothing\n",
        "end"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ux8CpqPjQeo3",
        "outputId": "b7e0f196-8ae0-47e0-c879-258c68164002"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "test2 (generic function with 1 method)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "struct Model3{T, S}\n",
        "    n::S\n",
        "    vin::T\n",
        "    vout::T\n",
        "end\n",
        "\n",
        "import Adapt\n",
        "function Adapt.adapt_structure(to, model::Model3)\n",
        "    n = Adapt.adapt_structure(to, model.n)\n",
        "    vin = Adapt.adapt_structure(to, model.vin)\n",
        "    vout = Adapt.adapt_structure(to, model.vout)\n",
        "    Model3(n, vin, vout)\n",
        "end"
      ],
      "metadata": {
        "id": "8y_61Jy6Qnpj"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = 10\n",
        "yin = CuArray(rand(n))\n",
        "yout = CuArray(zeros(n))\n",
        "m3 = Model3(n, yin, yout)\n",
        "[m3.vin m3.vout]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZPC_ZeAT8sj",
        "outputId": "c5527322-4dfd-4e35-bccd-522d671b672f"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10×2 CuArray{Float64, 2, CUDA.Mem.DeviceBuffer}:\n",
              " 0.727961  0.0\n",
              " 0.672124  0.0\n",
              " 0.454851  0.0\n",
              " 0.154048  0.0\n",
              " 0.217869  0.0\n",
              " 0.625703  0.0\n",
              " 0.807248  0.0\n",
              " 0.16296   0.0\n",
              " 0.669908  0.0\n",
              " 0.612245  0.0"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@cuda threads=m3.n test2(m3)\n",
        "[m3.vin m3.vout]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxaqyLCIRB6o",
        "outputId": "7c407beb-7c68-490b-dd0a-363ebac7ec84"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10×2 CuArray{Float64, 2, CUDA.Mem.DeviceBuffer}:\n",
              " 0.727961  7.27961\n",
              " 0.672124  6.72124\n",
              " 0.454851  4.54851\n",
              " 0.154048  1.54048\n",
              " 0.217869  2.17869\n",
              " 0.625703  6.25703\n",
              " 0.807248  8.07248\n",
              " 0.16296   1.6296\n",
              " 0.669908  6.69908\n",
              " 0.612245  6.12245"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://discourse.julialang.org/t/arrays-of-arrays-and-arrays-of-structures-in-cuda-kernels-cause-random-errors/69739"
      ],
      "metadata": {
        "id": "Dmyvat_OUYy4"
      }
    }
  ]
}